{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 104884,
          "sourceType": "datasetVersion",
          "datasetId": 54339
        }
      ],
      "dockerImageVersionId": 30732,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "notebook4148565786",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Choy98/Mestizo-Restaurant-web/blob/main/notebook4148565786.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'skin-cancer-mnist-ham10000:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F54339%2F104884%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240615%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240615T032528Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D61ec4b3a6973a1945d02bcb736767e76dd8205b09355bbaf8997fcfcfcc5d23571a85db48c80b9a5f1f2c089416a7e6b5df53a992f7bd605a5fc34e93adf4b4558e47c133b5be7985c21f79b44bf7a70f03e0079abd8592b6d3ba1b3c2fc539ff221877d7feba38b0758087d14c6bbcf08e82cd77fa334b9de573ee2cd30cdcf38a783524f238aae7e29bc8277716c64ae3fe39dfa24752e9db9632ed40088ca99db9c03ab417ed96178ed8c792458283fed2c098d84e24d8f4d7eda6b778058bb30722d902204359f9078d24bae2f325dbc35da617d3ef80d747694fc47e8a78f9e20e1ae7278810fd8efbbd725f8acf34f3f6d667019fab3c3614af6876f3b'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "K1evv0wb7c2a"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Loading Used Libriries**"
      ],
      "metadata": {
        "id": "e55yz6db7c2e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import matplotlib.pyplot as plt\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "import random\n",
        "import os\n",
        "import torch ## PyTorch\n",
        "import torch.nn as nn ## Neural networks package\n",
        "from torch import optim ## optimizer\n",
        "from torchvision import models ## package consists of popular datasets, model architectures, and common image transformations for computer vision.\n",
        "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
        "import torchvision.transforms as transforms ## for data augmentation\n",
        "from torchvision.transforms import v2 ## for data augmentation\n",
        "from sklearn.model_selection import train_test_split ## to split datasets\n",
        "\n",
        "\n",
        "\n",
        "# Setting the random seed for reproducibility\n",
        "seed = 77\n",
        "np.random.seed(seed) ## for numpy\n",
        "torch.manual_seed(seed) ## for PyTorch\n",
        "torch.cuda.manual_seed(seed)\n",
        "random.seed(24) ## for random module on python\n",
        "\n",
        "\n",
        "# Check if a GPU is available\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU is available.\")\n",
        "    num_gpus = torch.cuda.device_count()\n",
        "    print(f\"Number of GPUs available: {num_gpus}\")\n",
        "\n",
        "    for i in range(num_gpus):\n",
        "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
        "else:\n",
        "    print(\"GPU is not available.\")\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") ## use gpu if its available"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-14T03:35:30.858594Z",
          "iopub.execute_input": "2024-06-14T03:35:30.859477Z",
          "iopub.status.idle": "2024-06-14T03:35:37.287467Z",
          "shell.execute_reply.started": "2024-06-14T03:35:30.859442Z",
          "shell.execute_reply": "2024-06-14T03:35:37.286478Z"
        },
        "trusted": true,
        "id": "ElvAAyh_7c2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1 - EDA of HAM10000 Dataset**"
      ],
      "metadata": {
        "id": "zMgm9u3n7c2g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **1.1 - Loading Data**"
      ],
      "metadata": {
        "id": "rxmTLdtx7c2g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Path to data\n",
        "HAM1000_path = '/kaggle/input/skin-cancer-mnist-ham10000/'\n",
        "\n",
        "## to map the class acronym to its real name, according to the paper of the dataset\n",
        "lesion_type_dict = {\n",
        "    'nv': 'Melanocytic nevi',\n",
        "    'mel': 'Melanoma',\n",
        "    'bkl': 'Benign keratosis-like lesions ',\n",
        "    'bcc': 'Basal cell carcinoma',\n",
        "    'akiec': 'Actinic keratoses',\n",
        "    'vasc': 'Vascular lesions',\n",
        "    'df': 'Dermatofibroma'\n",
        "}\n",
        "\n",
        "## collecting image paths\n",
        "all_image_path = glob(os.path.join(HAM1000_path, '*', '*.jpg'))\n",
        "\n",
        "## imageid_path_dict will map each image ID (the file name without the extension) to its full file path.\n",
        "imageid_path_dict = {os.path.splitext(os.path.basename(x))[0]: x for x in all_image_path}\n",
        "\n",
        "\n",
        "df_original = pd.read_csv(os.path.join(HAM1000_path, 'HAM10000_metadata.csv'))\n",
        "df_original['path'] = df_original['image_id'].map(imageid_path_dict.get)\n",
        "df_original['cell_type'] = df_original['dx'].map(lesion_type_dict.get)\n",
        "df_original['cell_type_idx'] = pd.Categorical(df_original['cell_type']).codes\n",
        "\n",
        "## class names\n",
        "classes = df_original['dx'].unique()\n",
        "\n",
        "df_original.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-14T03:35:37.289365Z",
          "iopub.execute_input": "2024-06-14T03:35:37.290169Z",
          "iopub.status.idle": "2024-06-14T03:35:38.665406Z",
          "shell.execute_reply.started": "2024-06-14T03:35:37.290132Z",
          "shell.execute_reply": "2024-06-14T03:35:38.664416Z"
        },
        "trusted": true,
        "id": "-eZLpglT7c2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **1.2 - Data Distribution**"
      ],
      "metadata": {
        "id": "UJCsG6pM7c2h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of images in each class\n",
        "class_counts = df_original['cell_type'].value_counts()\n",
        "\n",
        "# Plot a histogram\n",
        "plt.figure(figsize=(10, 6))\n",
        "class_counts.plot(kind='barh')\n",
        "\n",
        "# Add title and labels\n",
        "plt.title('Number of Images per Class')\n",
        "plt.ylabel('Class')\n",
        "plt.xlabel('Number of Images')\n",
        "\n",
        "# Display the plot\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print(class_counts)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-14T03:35:38.666671Z",
          "iopub.execute_input": "2024-06-14T03:35:38.667495Z",
          "iopub.status.idle": "2024-06-14T03:35:39.093714Z",
          "shell.execute_reply.started": "2024-06-14T03:35:38.667461Z",
          "shell.execute_reply": "2024-06-14T03:35:39.092665Z"
        },
        "trusted": true,
        "id": "CBEn6PSr7c2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### We have a clearly prevalence of the Melanocytic nevi class but, more generally, it is possible to notice an imbalance of the classes samples."
      ],
      "metadata": {
        "id": "F9d19Tbi7c2i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **1.3 Samples Example**"
      ],
      "metadata": {
        "id": "IPMPN4QT7c2i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "## Showing some examples of the data\n",
        "\n",
        "# Randomly select 15 rows from the dataframe\n",
        "sample_df = df_original.sample(n=15)\n",
        "\n",
        "# Set up the matplotlib figure and axes\n",
        "fig, axes = plt.subplots(3, 5, figsize=(20, 12))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for ax, (_, row) in zip(axes, sample_df.iterrows()):\n",
        "    img_path = row['path']\n",
        "    cell_type = row['cell_type']\n",
        "\n",
        "    # Open the image\n",
        "    img = Image.open(img_path)\n",
        "\n",
        "    # Display the image on the axis\n",
        "    ax.imshow(img)\n",
        "    ax.set_title(cell_type)\n",
        "    ax.axis('off')  # Hide the axes ticks\n",
        "\n",
        "# Add a main title for the whole plot\n",
        "plt.suptitle('HAM10000 Examples', fontsize=20)\n",
        "# Adjust the layout\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-14T03:35:39.09603Z",
          "iopub.execute_input": "2024-06-14T03:35:39.096356Z",
          "iopub.status.idle": "2024-06-14T03:35:42.925993Z",
          "shell.execute_reply.started": "2024-06-14T03:35:39.096329Z",
          "shell.execute_reply": "2024-06-14T03:35:42.924911Z"
        },
        "trusted": true,
        "id": "Y9h-ajsi7c2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2 - Data Pre-processing**"
      ],
      "metadata": {
        "id": "TOpwjMBq7c2i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **2.1 - Useful Functions**"
      ],
      "metadata": {
        "id": "0nKSsQKw7c2j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Plots the ROC curve\n",
        "def plot_roc_curve(fpr, tpr, roc_auc):\n",
        "    plt.figure()\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "## gets the mean and std from a given dataset\n",
        "def calculate_mean_std_from_df(df, path_column):\n",
        "    transform = transforms.ToTensor()\n",
        "\n",
        "    mean = np.zeros(3)\n",
        "    std = np.zeros(3)\n",
        "    nb_samples = 0\n",
        "\n",
        "    for img_path in df[path_column]:\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        img = transform(img)\n",
        "        img = img.view(3, -1)\n",
        "        mean += img.mean(dim=1).numpy()\n",
        "        std += img.std(dim=1).numpy()\n",
        "        nb_samples += 1\n",
        "\n",
        "    mean /= nb_samples\n",
        "    std /= nb_samples\n",
        "\n",
        "    return mean, std\n",
        "\n",
        "## Define the custom dataset class\n",
        "class CustomHAM10000(Dataset):\n",
        "    def __init__(self, dataframe, img_size, transform=None):\n",
        "        self.paths = dataframe['path'].values\n",
        "        self.labels = torch.tensor(dataframe['cell_type_idx'].values, dtype=torch.long)\n",
        "        self.transform = transform\n",
        "        self.img_size = img_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.paths[idx]\n",
        "        image = Image.open(img_path)\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-14T03:35:42.927286Z",
          "iopub.execute_input": "2024-06-14T03:35:42.927606Z",
          "iopub.status.idle": "2024-06-14T03:35:42.94089Z",
          "shell.execute_reply.started": "2024-06-14T03:35:42.92758Z",
          "shell.execute_reply": "2024-06-14T03:35:42.93977Z"
        },
        "trusted": true,
        "id": "CBzNnOmG7c2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **2.2 - Split Data**"
      ],
      "metadata": {
        "id": "jUjenY5K7c2j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Setting Hyperparameters\n",
        "\n",
        "img_size = (120,120)  ## Used by https://arxiv.org/pdf/2303.07520\n",
        "batch_size = 32  ## Default\n",
        "random_state = seed"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-14T03:35:42.942205Z",
          "iopub.execute_input": "2024-06-14T03:35:42.942553Z",
          "iopub.status.idle": "2024-06-14T03:35:42.956387Z",
          "shell.execute_reply.started": "2024-06-14T03:35:42.942525Z",
          "shell.execute_reply": "2024-06-14T03:35:42.955282Z"
        },
        "trusted": true,
        "id": "kn7Zarqy7c2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## separates between training (full and reduced), validation (the reference paper doesnt use test set)\n",
        "df_train, df_val = train_test_split(df_original, test_size=0.11, stratify=df_original['dx'], random_state=random_state)\n",
        "#df_train, df_val = train_test_split(df_train_val, test_size=0.2, stratify=df_train_val['dx'], random_state=random_state)\n",
        "df_train_small, _ = train_test_split(df_train, train_size=0.25, stratify=df_train['dx'], random_state=random_state)\n",
        "\n",
        "\n",
        "## see datasets infos\n",
        "print('-'*30)\n",
        "print('datasets division')\n",
        "print('-'*30)\n",
        "print(f'Nº train (small) = {len(df_train_small)}')\n",
        "print(f'Nº train (full)  = {len(df_train)}')\n",
        "print(f'Nº val           = {len(df_val)}')\n",
        "#print(f'Nº test          = {len(df_test)}')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-14T03:35:42.957562Z",
          "iopub.execute_input": "2024-06-14T03:35:42.957825Z",
          "iopub.status.idle": "2024-06-14T03:35:43.004001Z",
          "shell.execute_reply.started": "2024-06-14T03:35:42.957802Z",
          "shell.execute_reply": "2024-06-14T03:35:43.00305Z"
        },
        "trusted": true,
        "id": "ACDOOT6T7c2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### The test_size was chosen to match the number of samples present on the validation set present on: https://ieeexplore.ieee.org/document/10020302."
      ],
      "metadata": {
        "id": "-VAEIok67c2k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_train['cell_type'].value_counts())\n",
        "print('-'*40)\n",
        "print(df_train_small['cell_type'].value_counts())\n",
        "print('-'*40)\n",
        "print(df_val['cell_type'].value_counts())\n",
        "#print('-'*40)\n",
        "#print(df_test['cell_type'].value_counts())\n",
        "#print('-'*40)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-14T03:35:43.005194Z",
          "iopub.execute_input": "2024-06-14T03:35:43.005505Z",
          "iopub.status.idle": "2024-06-14T03:35:43.016168Z",
          "shell.execute_reply.started": "2024-06-14T03:35:43.00548Z",
          "shell.execute_reply": "2024-06-14T03:35:43.015294Z"
        },
        "trusted": true,
        "id": "IkcV9Kxy7c2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### In order to evaluate the model realistically, the validation set will also present an imbalance. We define a small dataset to test different hyperparameters and techniques with time efficiency."
      ],
      "metadata": {
        "id": "AV3soyhR7c2k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **2.3 - Data Augmentation**"
      ],
      "metadata": {
        "id": "Ww34KuEu7c2k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "## Applying data augmentation on classes with lower prevalence\n",
        "\n",
        "lower_prevalence_df = df_train[df_train['cell_type'] != 'Melanocytic nevi']\n",
        "high_prevalence_df = df_train[df_train['cell_type'] == 'Melanocytic nevi']\n",
        "\n",
        "#lower_prevalence_df_small = df_train_small[df_train_small['cell_type'] != 'Melanocytic nevi']\n",
        "#high_prevalence_df_small = df_train_small[df_train_small['cell_type'] == 'Melanocytic nevi']\n",
        "\n",
        "\n",
        "## getting the mean and std from the training data\n",
        "mean_train, std_train = calculate_mean_std_from_df(df_train, 'path')\n",
        "#mean_train_small, std_train_small = calculate_mean_std_from_df(df_train_small, 'path')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-14T03:35:43.017409Z",
          "iopub.execute_input": "2024-06-14T03:35:43.01765Z",
          "iopub.status.idle": "2024-06-14T03:36:32.601913Z",
          "shell.execute_reply.started": "2024-06-14T03:35:43.017629Z",
          "shell.execute_reply": "2024-06-14T03:36:32.600882Z"
        },
        "trusted": true,
        "id": "uv4FfZqg7c2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Configuring transformer that will perform the Data Augmentation\n",
        "\n",
        "## transformations to the augmented classes\n",
        "transform_low_prev_class = transforms.Compose([\n",
        "                        transforms.RandomRotation(15), transforms.RandomHorizontalFlip(),transforms.RandomVerticalFlip(),\n",
        "                        transforms.RandomAffine(degrees=0, translate=(0.2,0.2)), ##random width height shift\n",
        "                        transforms.RandomResizedCrop(img_size, scale=(0.7, 1.0)), ##random zoom\n",
        "                        transforms.ColorJitter(brightness=0.2, contrast=0.2), ## bright and contrast adjustment\n",
        "                        transforms.Resize(img_size),\n",
        "                        transforms.ToTensor(), transforms.Normalize(mean_train, std_train)])\n",
        "\n",
        "## transformations for the rest of the data\n",
        "normal_transform = transforms.Compose([transforms.Resize(img_size),\n",
        "                                         transforms.ToTensor(),\n",
        "                                         transforms.Normalize(mean_train, std_train)])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-14T03:36:32.605227Z",
          "iopub.execute_input": "2024-06-14T03:36:32.605783Z",
          "iopub.status.idle": "2024-06-14T03:36:32.612997Z",
          "shell.execute_reply.started": "2024-06-14T03:36:32.605756Z",
          "shell.execute_reply": "2024-06-14T03:36:32.612073Z"
        },
        "trusted": true,
        "id": "MdHbFi1p7c2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **2.4 - Creating the Datasets**"
      ],
      "metadata": {
        "id": "yrKk14Xp7c2l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Creates the datasets\n",
        "\n",
        "## train_small_dataset\n",
        "lowprev_classes_augmented = CustomHAM10000(lower_prevalence_df, img_size, transform=transform_low_prev_class) ## applies data aug only on minority classes\n",
        "highprev_class_data = CustomHAM10000(high_prevalence_df, img_size, transform=normal_transform)\n",
        "\n",
        "#train_data_small= ConcatDataset([lowprev_classes_augmented, highprev_class_data])\n",
        "\n",
        "#ds_train_small = DataLoader(train_data_small, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "\n",
        "## train_dataset\n",
        "\n",
        "train_data= ConcatDataset([lowprev_classes_augmented, highprev_class_data])\n",
        "\n",
        "ds_train = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "\n",
        "\n",
        "## val_dataset\n",
        "\n",
        "val_data = CustomHAM10000(df_val, img_size, transform=normal_transform)\n",
        "\n",
        "ds_val = DataLoader(val_data, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True) ## Shuffle = False (Reproducibility)\n",
        "\n",
        "\n",
        "## test_dataset\n",
        "#test_data = CustomHAM10000(df_test, img_size, transform=normal_transform)\n",
        "#ds_test = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-14T03:36:32.613987Z",
          "iopub.execute_input": "2024-06-14T03:36:32.614336Z",
          "iopub.status.idle": "2024-06-14T03:36:32.642579Z",
          "shell.execute_reply.started": "2024-06-14T03:36:32.614302Z",
          "shell.execute_reply": "2024-06-14T03:36:32.641653Z"
        },
        "trusted": true,
        "id": "M9MVG3D-7c2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Now, we applied data augmentation to the classes with lower prevalence and created the datasets that the models will be trained and tested on. It is important to notice that the data is normalized, which is a good practice since it enhances performance. The Data Augmentation technique should reduce (but not eliminate) the imbalance problem that we detected earlier in our analysis."
      ],
      "metadata": {
        "id": "-Dxz-Gp67c2l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3 - Models' Experiments**"
      ],
      "metadata": {
        "id": "yMBLfzvS7c2l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **3.1 - Importing Pretrained ImageNet Models**"
      ],
      "metadata": {
        "id": "r4JKMLd87c2l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Function to load the models\n",
        "\n",
        "def initialize_net_arch(model_name, num_classes, input_size, feature_extract, use_pretrained = True):\n",
        "\n",
        "    ## stores the feature extractor of the selected model\n",
        "    model_backbone = None\n",
        "\n",
        "\n",
        "    if model_name == 'vgg16':\n",
        "\n",
        "        model_backbone = models.vgg16(pretrained=True)\n",
        "        #set_parameter_requires_grad(model_ft, feature_extract) ## for fine tunning\n",
        "\n",
        "        ## new FC layer\n",
        "        num_ftrs = model_backbone.classifier[6].in_features\n",
        "        model_backbone.classifier[6] = nn.Sequential(\n",
        "            nn.Linear(num_ftrs, 4096), ## dense layer\n",
        "            nn.ReLU(), ## relu activation\n",
        "            nn.Dropout(p=0.5), ## dropout with p probability\n",
        "            nn.Linear(4096, num_classes)\n",
        "            #nn.Softmax(dim=1) ## softmax activation (no need because of the loss used CrossEntropy)\n",
        "        )\n",
        "\n",
        "\n",
        "    elif model_name == 'resnet50':\n",
        "\n",
        "        model_backbone = models.resnet50(pretrained=use_pretrained)\n",
        "        #set_parameter_requires_grad(model_backbone, feature_extract) ## for fine tuning\n",
        "\n",
        "        ## new FC layer\n",
        "        num_ftrs = model_backbone.fc.in_features\n",
        "        model_backbone.fc = nn.Sequential(\n",
        "            nn.Linear(num_ftrs, 4096), ## dense layer\n",
        "            nn.ReLU(), ## relu activation\n",
        "            nn.Dropout(p=0.5), ## dropout with p probability\n",
        "            nn.Linear(4096, num_classes)\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "    elif model_name == 'inceptionV3':\n",
        "        '''Beware, the InceptionV3 uses Nx3x299x299 inputs, so you must resize your images to match that'''\n",
        "        model_backbone = models.inception_v3(pretrained=use_pretrained)\n",
        "        #set_parameter_requires_grad(model_backbone, feature_extract) ## for fine tuning\n",
        "\n",
        "        ## new FC layer\n",
        "\n",
        "        ## handle the auxilary net\n",
        "        num_ftrs = model_backbone.AuxLogits.fc.in_features\n",
        "        model_backbone.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
        "\n",
        "        ## handle the primary net\n",
        "        num_ftrs = model_backbone.fc.in_features\n",
        "        model_backbone.fc = nn.Sequential(\n",
        "            nn.Linear(num_ftrs, 4096), ## dense layer\n",
        "            nn.ReLU(), ## relu activation\n",
        "            nn.Dropout(p=0.5), ## dropout with p probability\n",
        "            nn.Linear(4096, num_classes)\n",
        "        )\n",
        "\n",
        "    elif model_name == 'densenet':\n",
        "        ## choosing the 161 model because it's closer to the inception performance on the ImageNet dataset\n",
        "\n",
        "        model_backbone = models.densenet161(pretrained=use_pretrained)\n",
        "\n",
        "        ## new FC layer\n",
        "        num_ftrs = model_backbone.classifier.in_features\n",
        "        model_backbone.classifier = nn.Sequential(\n",
        "            nn.Linear(num_ftrs, 4096), ## dense layer\n",
        "            nn.ReLU(), ## relu activation\n",
        "            nn.Dropout(p=0.5), ## dropout with p probability\n",
        "            nn.Linear(4096, num_classes)\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "    else:\n",
        "        print(\"Model name not listed, exiting...\\n\\n\")\n",
        "        exit()\n",
        "\n",
        "\n",
        "    return model_backbone\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-14T03:36:32.643776Z",
          "iopub.execute_input": "2024-06-14T03:36:32.644073Z",
          "iopub.status.idle": "2024-06-14T03:36:32.656862Z",
          "shell.execute_reply.started": "2024-06-14T03:36:32.644049Z",
          "shell.execute_reply": "2024-06-14T03:36:32.656105Z"
        },
        "trusted": true,
        "id": "-abzWlPE7c2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Setting some parameters\n",
        "\n",
        "num_classes = 7\n",
        "feature_extract = True\n",
        "\n",
        "## Used by https://arxiv.org/pdf/2303.07520\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "epochs = 30"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-14T03:36:32.657874Z",
          "iopub.execute_input": "2024-06-14T03:36:32.658139Z",
          "iopub.status.idle": "2024-06-14T03:36:32.670835Z",
          "shell.execute_reply.started": "2024-06-14T03:36:32.658117Z",
          "shell.execute_reply": "2024-06-14T03:36:32.669997Z"
        },
        "trusted": true,
        "id": "DX0I2kWC7c2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **3.2 - Defining the Metrics Used**"
      ],
      "metadata": {
        "id": "cfmTtMxC7c2m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Functions of the metrics\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_curve, auc\n",
        "\n",
        "\n",
        "## Accuracy function\n",
        "def calculate_accuracy(predictions, labels):\n",
        "    correct = (predictions == labels).sum().item()\n",
        "    total = labels.size(0)\n",
        "    return correct / total\n",
        "\n",
        "## Normalized Multi-class Accuracy function  (Ref -> https://challenge.isic-archive.com/landing/2019/)\n",
        "def normalized_multiclass_accuracy(predictions, labels, num_classes):\n",
        "    recall_per_class = []\n",
        "\n",
        "    for i in range(num_classes):\n",
        "        true_positives = ((predictions == i) & (labels == i)).sum().item()\n",
        "        total_positives = (labels == i).sum().item()\n",
        "\n",
        "        if total_positives > 0:\n",
        "            recall_per_class.append(true_positives / total_positives)\n",
        "        else:\n",
        "            recall_per_class.append(0.0)\n",
        "\n",
        "    normalized_accuracy = sum(recall_per_class) / num_classes\n",
        "    return normalized_accuracy\n",
        "\n",
        "## Funcion that calculates the remaining metrics\n",
        "def calculate_other_metrics(predictions, labels):\n",
        "    precision = precision_score(labels.cpu(), predictions.cpu(), average='weighted')\n",
        "    recall = recall_score(labels.cpu(), predictions.cpu(), average='weighted')\n",
        "    f1 = f1_score(labels.cpu(), predictions.cpu(), average='weighted')\n",
        "\n",
        "    # Compute ROC curve and AUC for each class\n",
        "    num_classes = len(torch.unique(labels))\n",
        "    fpr = []\n",
        "    tpr = []\n",
        "    roc_auc = []\n",
        "    for class_idx in range(num_classes):\n",
        "        class_labels = (labels.cpu() == class_idx)\n",
        "        class_predictions = (predictions.cpu() == class_idx)\n",
        "        fpr_i, tpr_i, _ = roc_curve(class_labels, class_predictions, pos_label=1)\n",
        "        roc_auc_i = auc(fpr_i, tpr_i)\n",
        "        fpr.append(fpr_i)\n",
        "        tpr.append(tpr_i)\n",
        "        roc_auc.append(roc_auc_i)\n",
        "\n",
        "    # Calculate macro-average AUC\n",
        "    macro_auc = sum(roc_auc) / num_classes\n",
        "\n",
        "    return precision, recall, f1, macro_auc"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-14T03:36:32.671907Z",
          "iopub.execute_input": "2024-06-14T03:36:32.672153Z",
          "iopub.status.idle": "2024-06-14T03:36:32.684788Z",
          "shell.execute_reply.started": "2024-06-14T03:36:32.672132Z",
          "shell.execute_reply": "2024-06-14T03:36:32.683915Z"
        },
        "trusted": true,
        "id": "cJQSI1Wd7c2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **3.3 - Defining the Training/Evalutation Loop**"
      ],
      "metadata": {
        "id": "S1mf93C47c2m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Training epoch function\n",
        "def train_epoch(model, optimizer, data_loader, loss_criterion, num_classes):\n",
        "    '''Train a Neural Network for one epoch'''\n",
        "    dev = next(model.parameters()).device ## gets where the model is alocated\n",
        "    model.train() ## tells that we're training so Dropout, etc will be active.\n",
        "\n",
        "    running_loss = 0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    for images, labels in data_loader:\n",
        "        images, labels = images.to(dev), labels.to(dev)\n",
        "        optimizer.zero_grad() ## cleans the gradient tensor (it accumulates)\n",
        "        output = model(images) ## get prediction of the model\n",
        "        loss = loss_criterion(output, labels) ## calculate loss\n",
        "        loss.backward() ## backprop to obtain the influence of the weights on the loss func\n",
        "        optimizer.step() ## updates the weights\n",
        "\n",
        "        running_loss += loss.item() ## accumulates the loss to calculate the average loss.\n",
        "\n",
        "        _, predicted = torch.max(output, 1)\n",
        "        all_predictions.append(predicted)\n",
        "        all_labels.append(labels)\n",
        "\n",
        "    all_predictions = torch.cat(all_predictions)\n",
        "    all_labels = torch.cat(all_labels)\n",
        "    accuracy = calculate_accuracy(all_predictions, all_labels)\n",
        "    normalized_accuracy = normalized_multiclass_accuracy(all_predictions, all_labels, num_classes)\n",
        "\n",
        "    return running_loss / len(data_loader), accuracy, normalized_accuracy\n",
        "\n",
        "## Evaluation function\n",
        "def evaluate(model, data_loader, loss_criterion, num_classes):\n",
        "    '''Evaluate the model on the validation set'''\n",
        "    dev = next(model.parameters()).device\n",
        "    model.eval() ## tells that we're evalutating the model, so we don't use Dropout layers, etc\n",
        "\n",
        "    running_loss = 0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in data_loader:\n",
        "            images, labels = images.to(dev), labels.to(dev)\n",
        "            output = model(images)\n",
        "            loss = loss_criterion(output, labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(output, 1) ## gets the predicted class with the highest probability (MSP).\n",
        "            all_predictions.append(predicted)\n",
        "            all_labels.append(labels)\n",
        "\n",
        "    all_predictions = torch.cat(all_predictions)\n",
        "    all_labels = torch.cat(all_labels)\n",
        "    accuracy = calculate_accuracy(all_predictions, all_labels)\n",
        "    normalized_accuracy = normalized_multiclass_accuracy(all_predictions, all_labels, num_classes)\n",
        "\n",
        "    return running_loss / len(data_loader), accuracy, normalized_accuracy, all_predictions, all_labels\n",
        "\n",
        "\n",
        "## Training loop function\n",
        "def train_loop(model, optimizer, criterion, train_loader, val_loader, epochs, num_classes):\n",
        "\n",
        "    best_val_acc = 0\n",
        "    best_val_acc_epoch = 1\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "\n",
        "        train_loss, train_accuracy, train_normalized_accuracy = train_epoch(model, optimizer, train_loader, criterion, num_classes)\n",
        "        val_loss, val_accuracy, val_normalized_accuracy, val_predictions, val_labels = evaluate(model, val_loader, criterion, num_classes)\n",
        "\n",
        "        print(f\"Training Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train Normalized Accuracy: {train_normalized_accuracy:.4f}\")\n",
        "        print(f\"Validation Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val Normalized Accuracy: {val_normalized_accuracy:.4f}\")\n",
        "        print('--'*50)\n",
        "\n",
        "        if val_accuracy > best_val_acc:\n",
        "            best_val_acc = val_accuracy\n",
        "            best_val_acc_epoch = epoch\n",
        "\n",
        "    print(f'Best Results at epoch {best_val_acc_epoch}. Val Accuracy = {best_val_acc}\\n\\n\\n')\n",
        "\n",
        "    return val_predictions, val_labels"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-14T03:50:32.639094Z",
          "iopub.execute_input": "2024-06-14T03:50:32.639832Z",
          "iopub.status.idle": "2024-06-14T03:50:32.655591Z",
          "shell.execute_reply.started": "2024-06-14T03:50:32.6398Z",
          "shell.execute_reply": "2024-06-14T03:50:32.654636Z"
        },
        "trusted": true,
        "id": "uTuT1yIw7c2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **3.4 - Training and Evaluation of Models**"
      ],
      "metadata": {
        "id": "V7kb3SaW7c2m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Instantiating the models\n",
        "\n",
        "#vgg16 = initialize_net_arch('vgg16', num_classes, img_size, feature_extract, use_pretrained = True)\n",
        "\n",
        "#resnet50 = initialize_net_arch('resnet50', num_classes, img_size, feature_extract, use_pretrained = True)\n",
        "\n",
        "densenet = initialize_net_arch('densenet', num_classes, img_size, feature_extract, use_pretrained = True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-14T03:36:32.704543Z",
          "iopub.execute_input": "2024-06-14T03:36:32.705245Z",
          "iopub.status.idle": "2024-06-14T03:36:35.954865Z",
          "shell.execute_reply.started": "2024-06-14T03:36:32.705214Z",
          "shell.execute_reply": "2024-06-14T03:36:35.953815Z"
        },
        "trusted": true,
        "id": "N5lkjJqS7c2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Setting the optimizer to each network\n",
        "\n",
        "## Used by https://arxiv.org/pdf/2303.07520\n",
        "\n",
        "#optimizer_vgg16 = optim.Adam(vgg16.parameters(), lr=0.0001)\n",
        "#optimizer_resnet50 = optim.Adam(resnet50.parameters(), lr=0.0001)\n",
        "optimizer_densenet = optim.Adam(densenet.parameters(), lr=0.0001) ## lr=0.001 (paper)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-14T03:36:35.956074Z",
          "iopub.execute_input": "2024-06-14T03:36:35.956391Z",
          "iopub.status.idle": "2024-06-14T03:36:35.964406Z",
          "shell.execute_reply.started": "2024-06-14T03:36:35.956366Z",
          "shell.execute_reply": "2024-06-14T03:36:35.963321Z"
        },
        "trusted": true,
        "id": "gcjWKQTw7c2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Training/Evaluating on validation set vgg16\n",
        "\n",
        "## set model to devide\n",
        "#vgg16 = vgg16.to(device)\n",
        "\n",
        "#vgg16_val_predictions, vgg16_val_labels = train_loop(vgg16, optimizer_vgg16, criterion, ds_train_small, ds_val, epochs, num_classes)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-14T03:36:35.965594Z",
          "iopub.execute_input": "2024-06-14T03:36:35.965941Z",
          "iopub.status.idle": "2024-06-14T03:36:35.975115Z",
          "shell.execute_reply.started": "2024-06-14T03:36:35.965916Z",
          "shell.execute_reply": "2024-06-14T03:36:35.9742Z"
        },
        "trusted": true,
        "id": "3y3A9TzX7c2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Training/Evaluating on validation set resnet50\n",
        "\n",
        "## set model to devide\n",
        "#resnet50 = resnet50.to(device)\n",
        "\n",
        "#resnet50_val_predictions, resnet50_val_labels = train_loop(resnet50, optimizer_resnet50, criterion, ds_train_small, ds_val, epochs, num_classes)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-14T03:36:35.976457Z",
          "iopub.execute_input": "2024-06-14T03:36:35.9768Z",
          "iopub.status.idle": "2024-06-14T03:36:35.988183Z",
          "shell.execute_reply.started": "2024-06-14T03:36:35.976772Z",
          "shell.execute_reply": "2024-06-14T03:36:35.987391Z"
        },
        "trusted": true,
        "id": "QAsKFcz57c2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "## Training/Evaluating on validation set densenet\n",
        "\n",
        "## set model to devide\n",
        "densenet = densenet.to(device)\n",
        "\n",
        "densenet_val_predictions, densenet_val_labels = train_loop(densenet, optimizer_densenet, criterion, ds_train, ds_val, epochs, num_classes)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-14T03:50:35.917812Z",
          "iopub.execute_input": "2024-06-14T03:50:35.918533Z",
          "iopub.status.idle": "2024-06-14T03:58:23.057741Z",
          "shell.execute_reply.started": "2024-06-14T03:50:35.9185Z",
          "shell.execute_reply": "2024-06-14T03:58:23.056579Z"
        },
        "trusted": true,
        "id": "LcXM_u5w7c2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Seeing the other metrics\n",
        "\n",
        "#print('VGG-16\\n')\n",
        "\n",
        "#precision, recall, f1, roc_auc = calculate_other_metrics(vgg16_val_predictions, vgg16_val_labels)\n",
        "#print(f\"Weighted Precision: {precision:.4f}, Weighted Recall: {recall:.4f}, Weighted F1: {f1:.4f}, Avg-AUC: {roc_auc:.4f}\\n\\n\")\n",
        "\n",
        "\n",
        "#print('ResNet50\\n')\n",
        "#precision, recall, f1, roc_auc = calculate_other_metrics(resnet50_val_predictions, resnet50_val_labels)\n",
        "#print(f\"Weighted Precision: {precision:.4f}, Weighted Recall: {recall:.4f}, Weighted F1: {f1:.4f}, Avg-AUC: {roc_auc:.4f}\")\n",
        "\n",
        "print('Densenet\\n')\n",
        "precision, recall, f1, roc_auc = calculate_other_metrics(densenet_val_predictions, densenet_val_labels)\n",
        "print(f\"Weighted Precision: {precision:.4f}, Weighted Recall: {recall:.4f}, Weighted F1: {f1:.4f}, Avg-AUC: {roc_auc:.4f}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-14T03:58:51.641646Z",
          "iopub.execute_input": "2024-06-14T03:58:51.642049Z",
          "iopub.status.idle": "2024-06-14T03:58:51.665121Z",
          "shell.execute_reply.started": "2024-06-14T03:58:51.642014Z",
          "shell.execute_reply": "2024-06-14T03:58:51.664158Z"
        },
        "trusted": true,
        "id": "1N22S8lj7c2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "KKCw8THp7c2s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}